import os
import sys
import tempfile

import torch

import gradio as gr

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from landiff.diffusion.dif_infer import CogModelInferWrapper, VideoTask
from landiff.llm.llm_cfg import build_llm
from landiff.llm.llm_infer import ArModelInferWrapper, ARSampleCfg, CodeTask
from landiff.utils import save_video_tensor


# åˆå§‹åŒ–æ¨¡å‹
def initialize_models(llm_ckpt_path, diffusion_ckpt_path):
    print("ğŸ“ æ­£åœ¨åˆå§‹åŒ–LLMæ¨¡å‹...")
    llm_model_cfg = build_llm()
    llm_model = ArModelInferWrapper(llm_ckpt_path, llm_model_cfg)

    print("ğŸ¬ æ­£åœ¨åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹...")
    diffusion_model = CogModelInferWrapper(ckpt_path=diffusion_ckpt_path)

    return llm_model, diffusion_model


# ç”Ÿæˆè§†é¢‘
def generate_video(
    prompt, cfg_scale, motion_score, llm_model, diffusion_model, seed=42
):
    print(f"ğŸš€ å¼€å§‹å¤„ç†æç¤º: {prompt}")
    print(f"ğŸ² ä½¿ç”¨éšæœºç§å­: {seed}")

    # å°†LLMæ¨¡å‹ç§»åˆ°GPU
    llm_model = llm_model.cuda()

    # åˆ›å»ºä¸´æ—¶ä¿å­˜è·¯å¾„ï¼Œä½†ä¸è‡ªåŠ¨åˆ é™¤
    temp_dir = tempfile.mkdtemp()
    print(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {temp_dir}")

    save_path = os.path.join(temp_dir, "output")

    # ä½¿ç”¨LLMç”Ÿæˆè¯­ä¹‰ä»¤ç‰Œ
    code_task = CodeTask(
        save_file_name=f"{save_path}.npy",
        prompt=prompt,
        seed=seed,
        sample_cfg=ARSampleCfg(
            temperature=1.0,
            cfg=cfg_scale,
            motion_score=motion_score,
        ),
    )

    print("ğŸ§  LLMç”Ÿæˆè¯­ä¹‰ä»¤ç‰Œä¸­...")
    code_task = llm_model(code_task)
    semantic_token = code_task.result.reshape(-1)

    # å°†LLMç§»å›CPUï¼Œé‡Šæ”¾GPUå†…å­˜
    llm_model = llm_model.cpu()
    torch.cuda.empty_cache()

    # å°†æ‰©æ•£æ¨¡å‹ç§»è‡³GPU
    diffusion_model = diffusion_model.cuda()
    semantic_token = semantic_token.cuda()

    # ä½¿ç”¨è¯­ä¹‰ä»¤ç‰Œç”Ÿæˆè§†é¢‘
    video_path = f"{save_path}.mp4"
    video_task = VideoTask(
        save_file_name=video_path,
        prompt=prompt,
        seed=seed,
        fps=8,
        semantic_token=semantic_token,
    )

    print("ğŸ¥ æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†é¢‘ä¸­...")
    video_task = diffusion_model(video_task)
    video = video_task.result

    # ä¿å­˜è§†é¢‘
    save_video_tensor(video, video_path, fps=video_task.fps)
    print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆï¼Œä¿å­˜è‡³: {video_path}")

    # å°†æ‰©æ•£æ¨¡å‹ç§»å›CPUï¼Œé‡Šæ”¾GPUå†…å­˜
    diffusion_model = diffusion_model.cpu()
    torch.cuda.empty_cache()

    return video_path


# Gradioç•Œé¢
def create_ui(llm_model, diffusion_model):
    with gr.Blocks(title="LanDiff: æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # LanDiff: é›†æˆè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆ
            
            æä¾›ä¸€æ®µè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼ŒLanDiffå°†ä¸ºæ‚¨ç”Ÿæˆç›¸åº”çš„è§†é¢‘ã€‚
            """
        )

        with gr.Row():
            with gr.Column(scale=3):
                prompt = gr.Textbox(
                    label="æ–‡æœ¬æç¤º",
                    placeholder="è¯·è¾“å…¥è¯¦ç»†çš„åœºæ™¯æè¿°ï¼Œä¾‹å¦‚ï¼šä¸€åªæ£•è‰²å’Œæ£•è¤è‰²å£³çš„èœ—ç‰›åœ¨ä¸€å¼ ç»¿è‰²çš„è‹”è—“åºŠä¸Šçˆ¬è¡Œã€‚èœ—ç‰›çš„èº«ä½“æ˜¯ç°è¤è‰²çš„ï¼Œæœ‰ä¸¤ä¸ªçªå‡ºçš„è§¦è§’å‘å‰ä¼¸å±•ã€‚ç¯å¢ƒè¡¨æ˜è¿™æ˜¯ä¸€ä¸ªè‡ªç„¶çš„æˆ·å¤–ç¯å¢ƒï¼Œé‡ç‚¹æ˜¯èœ—ç‰›åœ¨è‹”è—“è¡¨é¢ä¸Šçš„ç§»åŠ¨ã€‚",
                    lines=5,
                )

                with gr.Row():
                    with gr.Column(scale=1):
                        cfg_scale = gr.Slider(
                            label="CFGå°ºåº¦",
                            minimum=1.0,
                            maximum=15.0,
                            value=7.5,
                            step=0.5,
                            info="æ§åˆ¶æ–‡æœ¬æç¤ºå¯¹ç”Ÿæˆå†…å®¹çš„å½±å“ç¨‹åº¦",
                        )

                    with gr.Column(scale=1):
                        motion_score = gr.Slider(
                            label="è¿åŠ¨åˆ†æ•°",
                            minimum=0.0,
                            maximum=1.0,
                            value=0.1,
                            step=0.1,
                            info="æ§åˆ¶è§†é¢‘ä¸­è¿åŠ¨çš„ç¨‹åº¦",
                        )

                with gr.Row():
                    seed = gr.Number(
                        label="éšæœºç§å­",
                        value=42,
                        precision=0,
                        info="è®¾ç½®éšæœºç§å­ä»¥è·å¾—å¯é‡å¤çš„ç»“æœ",
                    )
                    random_seed_btn = gr.Button("éšæœºç§å­", size="sm")

                generate_btn = gr.Button("ç”Ÿæˆè§†é¢‘", variant="primary")

            with gr.Column(scale=2):
                video_output = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
                status = gr.Markdown("ç­‰å¾…ç”Ÿæˆ...")

        # å¤„ç†å‡½æ•°
        def process(prompt, cfg, motion, seed_value):
            try:
                # yield "ğŸ”„ æ­£åœ¨ç”Ÿæˆè§†é¢‘ï¼Œè¯·ç¨å€™...", None

                video_path = generate_video(
                    prompt, cfg, motion, llm_model, diffusion_model, int(seed_value)
                )
                print("âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼")
                return "âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼", video_path
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
                return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}", None

        def get_random_seed():
            import random

            return int(random.randint(0, 2**31 - 1))

        random_seed_btn.click(fn=get_random_seed, inputs=[], outputs=[seed])

        generate_btn.click(
            fn=process,
            inputs=[prompt, cfg_scale, motion_score, seed],
            outputs=[status, video_output],
        )

        gr.Markdown(
            """
            ### ç¤ºä¾‹æç¤º:
            1. ä¸€åªæ£•è‰²å’Œæ£•è¤è‰²å£³çš„èœ—ç‰›åœ¨ä¸€å¼ ç»¿è‰²çš„è‹”è—“åºŠä¸Šçˆ¬è¡Œã€‚èœ—ç‰›çš„èº«ä½“æ˜¯ç°è¤è‰²çš„ï¼Œæœ‰ä¸¤ä¸ªçªå‡ºçš„è§¦è§’å‘å‰ä¼¸å±•ã€‚
            2. ä¸€ç‰‡å®é™çš„æµ·æ»©ï¼Œé‡‘è‰²çš„æ²™å­ä¸Šå·èµ·æ³¢æµªï¼Œæµ·æ°´æ˜¯æ¸…æ¾ˆçš„è“ç»¿è‰²ï¼Œä¸è¿œå¤„çš„è“å¤©ç›¸èåˆã€‚å‡ ç‰‡ç™½äº‘é£˜åœ¨å¤©ç©ºä¸­ã€‚
            3. åœ¨æ»¡æ˜¯ç§¯é›ªçš„æ·±å±±ä¸­ï¼Œä¸€æ¡åè¿œçš„ç«è½¦è½¨é“èœ¿èœ’ç©¿è¿‡ã€‚ä¸€åˆ—çº¢è‰²çš„ç«è½¦å†’ç€è’¸æ±½ï¼Œç¼“ç¼“å‰è¡Œï¼Œåœ¨é›ªåœ°ä¸Šç•™ä¸‹è½¨è¿¹ã€‚
            """
        )

        gr.Markdown(
            """
            ---
            ### å…³äºLanDiff
            LanDiffæ˜¯ä¸€ä¸ªé›†æˆè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿä»è¯¦ç»†çš„æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡è§†é¢‘å†…å®¹ã€‚
            
            [é¡¹ç›®ä¸»é¡µ](https://landiff.github.io/) | [GitHub](https://github.com/LanDiff/LanDiff) | [è®ºæ–‡](https://arxiv.org/abs/2503.04606)
            """
        )

    return demo


# ä¸»å‡½æ•°
def main():
    # æ¨¡å‹è·¯å¾„è®¾ç½®
    llm_ckpt_path = "ckpts/LanDiff/llm/model.safetensors"
    diffusion_ckpt_path = "ckpts/LanDiff/diffusion"

    print("ğŸ”§ æ­£åœ¨è®¾ç½®ç¯å¢ƒ...")
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…ˆåœ¨CPUä¸Šï¼‰
    llm_model, diffusion_model = initialize_models(llm_ckpt_path, diffusion_ckpt_path)

    # åˆ›å»ºå’Œå¯åŠ¨Gradioæ¥å£
    demo = create_ui(llm_model, diffusion_model)
    demo.queue(max_size=10).launch(server_name="0.0.0.0")


if __name__ == "__main__":
    main()
